<!-- Ibis: https://github.com/lks-ai/ibis -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Ibis: AI-Driven Web Interface</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 width=%2240%22 height=%2224%22 viewBox=%220 0 372 220%22 style=%22shape-rendering:geometricPrecision;text-rendering:geometricPrecision;image-rendering:optimizeQuality;fill-rule:evenodd;clip-rule:evenodd%22 fill=%22%23e5e5e5%22><path style=%22opacity:1%22 d=%22M-.5 188.5v-1a5114.883 5114.883 0 0 0 129.5-11l1.5-1c-5.379-17.652-8.045-35.652-8-54 17.903-.382 35.903-.382 54 0 .543-.06.876-.393 1-1-.825-.886-1.825-1.219-3-1a1003.579 1003.579 0 0 1-53.5-11 239.714 239.714 0 0 1 4.5-44 684.163 684.163 0 0 1 49 27.5c1.902 1.301 3.902 1.968 6 2l-46-38c-1.727-1.39-3.061-3.056-4-5 3.769-10.377 9.436-19.544 17-27.5a3180.312 3180.312 0 0 1 46.5 48c-1.332-2.98-2.999-5.98-5-9A3607.354 3607.354 0 0 1 154.5 17c8.396-7.116 17.896-12.283 28.5-15.5a2363.505 2363.505 0 0 1 35.5 96c-10.341 18.196-13.507 37.53-9.5 58 7.862 18.285 21.862 27.952 42 29 6.757-.282 13.59-1.115 20.5-2.5l24-8a42.479 42.479 0 0 1 13 0 14.587 14.587 0 0 1 6 4c12.596 1.317 24.596 4.65 36 10 8.401 4.04 13.234 10.54 14.5 19.5-11.297-10.726-24.797-16.059-40.5-16a2744.471 2744.471 0 0 0-101 10.5 1809.795 1809.795 0 0 0-41 9c-21.557 2.755-41.723-1.412-60.5-12.5a129.804 129.804 0 0 0-20.5-4.5 2176.774 2176.774 0 0 0-102-5.5Z%22/><path style=%22opacity:.941%22 d=%22M174.5 119.5c1.175-.219 2.175.114 3 1-.124.607-.457.94-1 1l-2-2Z%22/></svg>" type="image/svg+xml">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* Basic styles */
        body, html {
            margin: 0;
            padding: 0;
            overflow: hidden;
            user-select: none;
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 50px;
            overflow: auto;
        }
        #message-bar {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 50px;
            display: flex;
            border-top: 1px solid #ccc;
            background-color: #f9f9f9;
        }
        #message-input {
            flex: 1;
            padding: 10px;
            border: none;
            font-size: 16px;
        }
        #microphone-button {
            width: 50px;
            border: none;
            background-color: #007BFF;
            color: white;
            font-size: 20px;
            cursor: pointer;
            position: relative;
        }
        #microphone-button[disabled] {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        #send-button {
            width: 80px;
            border: none;
            background-color: #007BFF;
            color: white;
            font-size: 16px;
            cursor: pointer;
            position: relative;
        }
        #send-button[disabled] {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        #spinner {
            position: absolute;
            top: 14px;
            right: 30px;
            width: 20px;
            height: 20px;
            border: 2px solid #ffffff;
            border-top: 2px solid #007BFF;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            display: none;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .highlight-overlay {
            position: absolute;
            border: 2px solid #00FF00;
            pointer-events: none;
            z-index: 9999;
        }
    </style>
    <style id="core-styles"></style>
</head>
<body>

<div id="canvas"></div>

<div id="message-bar">
    <input type="text" id="message-input" placeholder="Ask it to make you something...">
    <button id="microphone-button" title="Hands-Free Mode">ðŸŽ¤</button>
    <button id="send-button" title="Send">
      <svg xmlns="http://www.w3.org/2000/svg" width="40" height="24" viewBox="0 0 372 220" style="shape-rendering:geometricPrecision;text-rendering:geometricPrecision;image-rendering:optimizeQuality;fill-rule:evenodd;clip-rule:evenodd" fill="currentColor">
        <path style="opacity:1" d="M-.5 188.5v-1a5114.883 5114.883 0 0 0 129.5-11l1.5-1c-5.379-17.652-8.045-35.652-8-54 17.903-.382 35.903-.382 54 0 .543-.06.876-.393 1-1-.825-.886-1.825-1.219-3-1a1003.579 1003.579 0 0 1-53.5-11 239.714 239.714 0 0 1 4.5-44 684.163 684.163 0 0 1 49 27.5c1.902 1.301 3.902 1.968 6 2l-46-38c-1.727-1.39-3.061-3.056-4-5 3.769-10.377 9.436-19.544 17-27.5a3180.312 3180.312 0 0 1 46.5 48c-1.332-2.98-2.999-5.98-5-9A3607.354 3607.354 0 0 1 154.5 17c8.396-7.116 17.896-12.283 28.5-15.5a2363.505 2363.505 0 0 1 35.5 96c-10.341 18.196-13.507 37.53-9.5 58 7.862 18.285 21.862 27.952 42 29 6.757-.282 13.59-1.115 20.5-2.5l24-8a42.479 42.479 0 0 1 13 0 14.587 14.587 0 0 1 6 4c12.596 1.317 24.596 4.65 36 10 8.401 4.04 13.234 10.54 14.5 19.5-11.297-10.726-24.797-16.059-40.5-16a2744.471 2744.471 0 0 0-101 10.5 1809.795 1809.795 0 0 0-41 9c-21.557 2.755-41.723-1.412-60.5-12.5a129.804 129.804 0 0 0-20.5-4.5 2176.774 2176.774 0 0 0-102-5.5Z"/>
        <path style="opacity:.941" d="M174.5 119.5c1.175-.219 2.175.114 3 1-.124.607-.457.94-1 1l-2-2Z"/>
      </svg>
    </button>
    <div id="spinner"></div>
</div>

<!-- API Key Modal -->
<div id="api-key-modal" style="display: none;">
    <div style="position: fixed; top: 0; left: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.5);">
        <div style="position: relative; width: 80%; max-width: 400px; margin: 100px auto; background: white; padding: 20px; border-radius: 8px;">
            <h2>Enter Your OpenAI API Key</h2>
            <p>You can obtain your API key from the <a href="https://platform.openai.com/account/api-keys" target="_blank">OpenAI Dashboard</a>.</p>
            <input type="text" id="api-key-input" style="width: 100%; padding: 10px; margin-bottom: 20px;" placeholder="sk-...">
            <p><b><small>Ibis only ever calls OpenAI and runs as a standalone HTML app.</small></b></p>
            <button id="save-api-key-button" style="padding: 10px 20px;">Begin</button>
        </div>
    </div>
</div>

<script>
    // Check for API Key in URL parameters
    const urlParams = new URLSearchParams(window.location.search);
    let apiKey = urlParams.get('apiKey') || '';

    // DOM Elements
    const canvas = document.getElementById('canvas');
    const messageBar = document.getElementById('message-bar');
    const messageInput = document.getElementById('message-input');
    const sendButton = document.getElementById('send-button');
    const microphoneButton = document.getElementById('microphone-button');
    const spinner = document.getElementById('spinner');
    const apiKeyModal = document.getElementById('api-key-modal');
    const apiKeyInput = document.getElementById('api-key-input');
    const saveApiKeyButton = document.getElementById('save-api-key-button');
    const styleElement = document.getElementById('core-styles');

    let audio = null;
    let selectedElement = null;
    let messageHistory = [];
    let elementMap = {}; // Map of element IDs to their descriptions and code
    let elementEmbeddings = {}; // Map of element IDs to their embeddings
    
    // Directives
    let directives = null;

    // Hands-Free Mode Variables
    let isHandsFree = false;
    let isListening = false;
    let latestUtteranceDiv = null;
    let stream = null;
    let audioContext = null;
    let source = null;
    let analyser = null;
    let dataArray = null;
    let mediaRecorder = null;

    // Show API Key Modal if apiKey is not set
    if (!apiKey) {
        apiKeyModal.style.display = 'block';
    }

    // Save API Key
    saveApiKeyButton.addEventListener('click', () => {
        apiKey = apiKeyInput.value.trim();
        if (apiKey) {
            apiKeyModal.style.display = 'none';
        }
    });

    // Element Selection Overlay
    const highlightOverlay = document.createElement('div');
    highlightOverlay.className = 'highlight-overlay';
    document.body.appendChild(highlightOverlay);

    // Select and Deselect Elements
    canvas.addEventListener('click', (event) => {
        if (event.target !== canvas && !messageBar.contains(event.target)) {
            if (selectedElement) {
                // Remove previous highlight
                highlightOverlay.style.display = 'none';
            }
            selectedElement = event.target;

            // Position the highlight overlay over the selected element
            const rect = selectedElement.getBoundingClientRect();
            highlightOverlay.style.width = rect.width + 'px';
            highlightOverlay.style.height = rect.height + 'px';
            highlightOverlay.style.left = rect.left + 'px';
            highlightOverlay.style.top = rect.top + 'px';
            highlightOverlay.style.display = 'block';

            console.log('Selected Element:', selectedElement);
        } else {
            if (selectedElement) {
                highlightOverlay.style.display = 'none';
                selectedElement = null;
            }
        }
    });

    // Send Message
    sendButton.addEventListener('click', () => sendMessage());
    messageInput.addEventListener('keydown', (event) => {
        if (event.key === 'Enter') {
            sendMessage();
        }
    });

    // Microphone Button
    microphoneButton.addEventListener('click', async () => {
        isHandsFree = !isHandsFree;
        updateUIForHandsFreeMode();
        if (isHandsFree) {
            startHandsFreeMode();
        } else {
            stopHandsFreeMode();
        }
    });

    function updateUIForHandsFreeMode() {
        if (isHandsFree) {
            // Change microphone button appearance to indicate hands-free mode
            microphoneButton.textContent = 'ðŸ›‘';
            microphoneButton.title = 'Stop Hands-Free Mode';

            // Modify message bar to show latest utterance and a button to switch back
            messageInput.style.display = 'none';
            sendButton.style.display = 'none';

            if (!latestUtteranceDiv) {
                latestUtteranceDiv = document.createElement('div');
                latestUtteranceDiv.id = 'latest-utterance';
                latestUtteranceDiv.style.flex = '1';
                latestUtteranceDiv.style.padding = '10px';
                latestUtteranceDiv.style.fontSize = '16px';
                latestUtteranceDiv.textContent = 'Listening...';
                messageBar.insertBefore(latestUtteranceDiv, microphoneButton.nextSibling);
            }

        } else {
            // Change microphone button back to normal
            microphoneButton.textContent = 'ðŸŽ¤';
            microphoneButton.title = 'Start Hands-Free Mode';

            // Restore message input and send button
            messageInput.style.display = '';
            sendButton.style.display = '';

            // Remove latest utterance display
            if (latestUtteranceDiv) {
                latestUtteranceDiv.remove();
                latestUtteranceDiv = null;
            }
        }
    }

    async function startHandsFreeMode() {
        isListening = true;
        try {
            stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            source = audioContext.createMediaStreamSource(stream);
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 2048;
            source.connect(analyser);
            dataArray = new Float32Array(analyser.fftSize);
            listenContinuously();
        } catch (error) {
            console.error('Microphone Access Error:', error);
            alert('An error occurred while accessing the microphone.');
            isListening = false;
            isHandsFree = false;
            updateUIForHandsFreeMode();
        }
    }

    function stopHandsFreeMode() {
        isListening = false;
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
        }
        if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
        }
        if (audioContext) {
            audioContext.close();
            audioContext = null;
        }
    }

    async function listenContinuously() {
        while (isListening) {
            try {
                const transcription = await transcribeSpeech();
                if (!isListening) break; // Check if listening was turned off during transcription
    
                // Check if transcription is valid (e.g., longer than 1 character)
                if (transcription && transcription.length > 1) {
                    latestUtteranceDiv.textContent = 'You: ' + transcription;
                    const result = await sendMessage(transcription);
                    latestUtteranceDiv.textContent += '\nAssistant: ' + (result.assistantComment || '[No comment]');
                } else {
                    latestUtteranceDiv.textContent = 'Listening...';
                }
            } catch (error) {
                console.error('Transcription Error:', error);
                alert('An error occurred during speech transcription.');
                break;
            }
        }
    }
    
    async function sendMessage(userMessageInput) {
        const userMessage = userMessageInput ? userMessageInput.trim() : messageInput.value.trim();
        if (!userMessage || !apiKey) return;
    
        sendButton.disabled = true;
        spinner.style.display = 'block';
    
        // Add user message to history
        const userTimestamp = new Date().toISOString();
        messageHistory.push({
            role: 'user',
            content: userMessage,
            code: null,
            comment: null,
            timestamp: userTimestamp
        });
        if (messageHistory.length > 24) {
            messageHistory.shift();
        }
    
        console.log('User Message:', userMessage);
    
        // Load existing page content and style
        let pageContent = document.querySelector('#canvas').innerHTML;
        let pageStyle = styleElement.innerText;
    
        // Get System State Information
        const currentDateTime = new Date().toLocaleString();
        const userAgent = navigator.userAgent;
    
        // Prepare System Prompt
        let systemPrompt = `
## Identity
You are an HTML and DOM manipulation expert named Ibis who's tasked with modifying elements on the page. You operate under the new user interface paradigm "What you ask is what you get".

## System State
- Current Date and Time: ${currentDateTime}
- User Agent: ${userAgent}

## Task Description
Respond to the user's request by providing JavaScript code that manipulates the DOM to achieve the desired outcome. Ensure that your code is safe and does not contain any malicious content.

### Current Page Content
\`\`\`html
${pageContent}
\`\`\`

### Current Page Style
\`\`\`css
${pageStyle}
\`\`\`

## Recent Message History
${messageHistory.slice(-6).map(msg => `**${msg.role}**: ${msg.content}`).join('\n')}

        `;
    
        if (selectedElement) {
            // Extract attributes of the selected element
            const attributes = Array.from(selectedElement.attributes)
                .map(attr => `${attr.name}: "${attr.value}"`)
                .join('\n');
    
            systemPrompt += `### Selected Element Attributes\n`;
            systemPrompt += `Tag Name: ${selectedElement.tagName}\n`;
            systemPrompt += `${attributes}\n`;
        }
    
        systemPrompt += `
## Instructions

### Directives and Objectives
${directives}

### Important Global JavaScript Variables
The following variables are already declared:
- canvas: The main div that holds the page content. This is what the user means by the page or content because this is what they are perceiving rather than the whole HTML of the page.
- apiKey: Holds the OpenAI API key if you need to use generative AI functionalities.
- styleElement: Holds the <style> element in the head of the page. For use with manipulating core page style.
- directives: This is a string which holds a prioritized markdown list of the Directives and Objectives section representing how you should interpret what the user is saying into some coding task which manipulates the DOM. If directives is null just follow instructions normally. An example might be presentation mode where you are supposed to interpret their utterances visually as animated SVGs or the like. Directives are high level user experience objectives.

### Global Interface functions
- stopHandsFreeMode(): call this function if the user asks to stop hands free mode or for you to stop listening in some way.
- startHandsFreeMode(): call this function if the user asks to start hands free mode or to use the microphone or use speech.
- generateImage(prompt): call this async function to generate an image. You need to write the prompt and it returns a URL you can place in a new image element.
- textToSpeech(text, voice): call this function if you wish to speak directly to the user with an important comment, tip or update about something, even as part of some app you are building together. Keep comments concise yet informative. two sentences max. voice defaults to "alloy" but use known voices for effect. voice can be "alloy", "fable", "onyx", "shimmer", "nova" or "echo".

### Commenting Instructions
- If you want to make a comment that should be spoken aloud, prefix it with "### Comment" in a section at the end of your reply.
- Regular coding or DOM comments or explanations **should not** be prefixed and **will not** be spoken aloud.
- Ensure that spoken comments are concise and relevant to the user's request or the changes made.
- If you plan to include code in your reply, your comments should be in past tense if you are talking about changes you have made.
- Assume the user doesn't care about code you've written, or how you've done things, speak to them as an end-user with no coding experience unless they direct you otherwise.
- If code is not included in your reply, only include a comment section.
- If the user is simply asking a question, just start your response with a "### Comment" header but remember to keep your answer concise and quit yapping.

### Coding Instructions
- Only ever write JavaScript code.
- Use only standard JavaScript and DOM APIs.
- External libraries may be used, but stick to known CDNs.
- Always write fully functional interface, including back-end support if necessary. Avoid writing simulations of apps or toy apps unless that is what the user explicity requests.
- If the user is asking you to change your directives, goals or the way you interpret what they say, change the directives variable to reflect those new interpretive goals for how you manipulate the DOM.
- Think carefully before deciding to write the code to change the directives variable string because it will affect how you interpret everything here and change the modality of user speech, thus their experience.
- Ensure the code is enclosed within \`\`\`javascript\`\`\` tags.
- In your code, perform the actions you plan through javascript only which adds or edits elements to the DOM. Avoid code sections for html because they will not be processed.
- Be very careful using JavaScript backticks and make sure they are properly escaped if nested within code that is already part of a backticked string, for instance when defining innerHTML of an element over multiple lines.
- If you define variables which should rightly be global context for some functionality across scripts or elements, add them to the already existing global window object
- If the user asks to download or save the page they mean a copy of the innerHTML of the div#canvas element with its own head and unique title elements, plus any JavaScript and style script needed to interact with it. Think of it as asking to download a self-contained app or page.
- Any elements you generate should have a unique id.
- If modifying an existing element, reference it by its id
- Avoid duplicating content by properly identifying content the user has already created and alter it. You can do this by first probing to see if an element already exists.
- Avoid dumping strings into innerHTML properties unless necessary.
- Focus on what changes need to be made to the page content when writing your code, instead of always completely replacing it.
- Only use generateImage if a user requests an image in their current request without specifying a URL.
- When positioning and styling new Image elements, make sure to take into account the layout context around it to make it fit correctly.
- If an image element already has a src URL from openai, avoid regenerating the image
- If the user does not specify any page styles, infer what style they want based on what you have been doing. Remember for visual clarity we need padding. Edit style by editing the global styleElement.
- Use built-in modals instead of alerts where applicable
- Never clear the entire body, only the content within the canvas element
- If you include JavaScript functionality for element events, this JS needs to be included in a <script> tag within the page content that has its own unique id.
- If the user asks to scroll the page you are actually scrolling the div#canvas element
- Only use your code from the chat history as a reference, avoiding re-generating things which are not essential to the current user request.
- Make sure to rewrite your directives variable if the user wants you to change the goal or objective of your conversation.
- Use an anonymous function closure and IIFE to enclose your code in a function which will not pollute the namespace.
        `;
    
        // Prepare API Request
        const requestBody = {
            model: 'gpt-4o',
            messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: userMessage }
            ]
        };
        console.log('System Prompt:', systemPrompt);
    
        // Call OpenAI API
        try {
            const response = await fetch('https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify(requestBody)
            });
            const data = await response.json();
    
            const assistantMessage = data.choices[0].message.content;
            console.log('Assistant Response:', assistantMessage);
    
            let code = '';
            let assistantComment = '';
    
            // Extract JavaScript code from the assistant's response
            const codeMatch = assistantMessage.match(/```javascript\n([\s\S]*?)\n```/);
            if (codeMatch && codeMatch[1]) {
                code = codeMatch[1];
    
                // Wrap the code in a try/catch block
                const wrappedCode = `
    try {
        ${code}
    } catch (e) {
        textToSpeech("Error: " + e.message);
    }
    `;
    
                // Extract text before and after the code block for comments
                const beforeCodeMatch = assistantMessage.match(/^(.*?)```javascript/);
                const afterCodeMatch = assistantMessage.match(/```javascript[\s\S]*?```(.*)$/);
    
                const beforeCode = beforeCodeMatch ? beforeCodeMatch[1].trim() : '';
                const afterCode = afterCodeMatch ? afterCodeMatch[1].trim() : '';
    
                // Identify comments intended for speech synthesis
                const commentRegex = /### Comment\s*([\s\S]*?)(?=###|```|$)/g;
                let commentMatches = [];
                let match;
                while ((match = commentRegex.exec(assistantMessage)) !== null) {
                    commentMatches.push(match[1].trim());
                }
    
                // Combine all extracted comments
                assistantComment = commentMatches.join(' ');
    
                // Execute the wrapped code within a script tag
                const script = document.createElement('script');
                script.textContent = wrappedCode;
                document.body.appendChild(script);
                document.body.removeChild(script);
            }
    
            // Add assistant message to history
            const assistantTimestamp = new Date().toISOString();
            messageHistory.push({
                role: 'assistant',
                content: assistantMessage,
                code: code,
                comment: assistantComment,
                timestamp: assistantTimestamp
            });
            if (messageHistory.length > 24) {
                messageHistory.shift();
            }
    
            // Call textToSpeech with assistantComment in a non-blocking way
            if (assistantComment) {
                textToSpeech(assistantComment).catch(err => {
                    console.error('Error in textToSpeech:', err);
                });
            }
    
            if (!isHandsFree) {
                messageInput.value = '';
            }
            sendButton.disabled = false;
            spinner.style.display = 'none';
    
            return { assistantComment };
        } catch (error) {
            console.error('Error:', error);
            alert('An error occurred while communicating with the OpenAI API.');
            sendButton.disabled = false;
            spinner.style.display = 'none';
        }
    }
            
    // Helper functions

    // Cosine similarity between two vectors
    function cosineSimilarity(a, b) {
        let dotProduct = 0.0;
        let normA = 0.0;
        let normB = 0.0;
        for (let i = 0; i < a.length; i++) {
            dotProduct += a[i] * b[i];
            normA += a[i] * a[i];
            normB += b[i] * b[i];
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    // Get embedding for a text using OpenAI Embeddings API
    async function getEmbedding(text) {
        const response = await fetch('https://api.openai.com/v1/embeddings', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                input: text,
                model: 'text-embedding-ada-002'
            })
        });
        const data = await response.json();
        return data.data[0].embedding;
    }

    // Function to transcribe speech with improved VAD and dynamic audio interruption
    async function transcribeSpeech() {
        return new Promise(async (resolve, reject) => {
            try {
                // Function to calculate the audio volume
                function getVolume() {
                    analyser.getFloatTimeDomainData(dataArray);
                    let sum = 0.0;
                    for (let i = 0; i < dataArray.length; i++) {
                        sum += dataArray[i] * dataArray[i];
                    }
                    const rms = Math.sqrt(sum / dataArray.length);
                    return rms;
                }

                // VAD parameters
                const silenceThreshold = 0.01; // Volume below which is considered silence
                const speechThreshold = 0.02;  // Volume above which is considered speech
                const silenceDuration = 1500;   // Duration (ms) of silence before stopping
                const speechDuration = 300;     // Duration (ms) of speech before starting
                let silenceStart = null;
                let speechStart = null;
                let recording = false;

                const checkInterval = 100; // Interval (ms) to check for silence/speech

                // Buffers for audio chunks
                let chunks = [];

                // Function to stop recording
                function stopRecording() {
                    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                        mediaRecorder.stop();
                    }
                }

                // Function to check for speech and silence
                function checkAudio() {
                    const volume = getVolume();
                    // console.log('Volume:', volume);

                    if (!recording) {
                        // Waiting to start recording
                        if (volume > speechThreshold) {
                            if (speechStart === null) {
                                speechStart = Date.now();
                            } else if (Date.now() - speechStart > speechDuration) {
                                // Speech detected long enough, interrupt any playing audio and start recording
                                if (audio && !audio.paused) {
                                    audio.pause();
                                    audio.currentTime = 0;
                                    console.log('Interrupted current audio playback due to detected speech.');
                                }
                                startRecording();
                            }
                        } else {
                            speechStart = null;
                        }
                    } else {
                        // Currently recording
                        if (volume < silenceThreshold) {
                            if (silenceStart === null) {
                                silenceStart = Date.now();
                            } else if (Date.now() - silenceStart > silenceDuration) {
                                // Silence detected long enough, stop recording
                                stopRecording();
                            }
                        } else {
                            silenceStart = null;
                        }
                    }
                }

                // Function to start recording
                function startRecording() {
                    recording = true;
                    speechStart = null;
                    silenceStart = null;
                    chunks = [];
                    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                    mediaRecorder.ondataavailable = e => {
                        if (e.data.size > 0) {
                            chunks.push(e.data);
                        }
                    };
                    mediaRecorder.onstop = async () => {
                        clearInterval(intervalId);
                        silenceStart = null;
                        recording = false;
                        const blob = new Blob(chunks, { type: 'audio/webm' });

                        // Check if audio duration is longer than minimum threshold
                        const duration = await getAudioDuration(blob);
                        if (duration >= 2.0) { // Minimum duration in seconds
                            try {
                                const transcription = await sendToWhisper(blob);
                                resolve(transcription);
                            } catch (error) {
                                reject(error);
                            }
                        } else {
                            // Ignore short recordings
                            resolve('');
                        }
                    };
                    mediaRecorder.start();
                    console.log('Recording started.');
                }

                // Start checking for audio levels at regular intervals
                const intervalId = setInterval(checkAudio, checkInterval);

            } catch (error) {
                reject(error);
            }
        });
    }
    
    async function generateImage(prompt) {
        try {
            const response = await fetch('https://api.openai.com/v1/images/generations', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify({
                    prompt: prompt,
                    n: 1,
                    size: '1024x1024',
                    response_format: 'url'
                })
            });
    
            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`OpenAI API error: ${response.status} ${response.statusText} - ${JSON.stringify(errorData)}`);
            }
    
            const data = await response.json();
            const imageUrl = data.data[0].url;
            return imageUrl;
        } catch (error) {
            console.error('Error generating image:', error);
            throw error;
        }
    }

    // Function to get audio duration
    async function getAudioDuration(blob) {
        return new Promise((resolve) => {
            const tempAudio = document.createElement('audio');
            tempAudio.src = URL.createObjectURL(blob);
            tempAudio.addEventListener('loadedmetadata', () => {
                resolve(tempAudio.duration);
            });
        });
    }

    // Function to send the audio blob to OpenAI's Whisper API
    async function sendToWhisper(blob) {
        const formData = new FormData();
        formData.append('file', blob, 'audio.webm');
        formData.append('model', 'whisper-1');

        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${apiKey}`,
            },
            body: formData,
        });

        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        const data = await response.json();
        return data.text;
    }

    // Function to convert text to speech with audio management
    async function textToSpeech(text, voice = 'alloy') {
        const MAX_CHAR = 4096;

        // Helper function to truncate text intelligently
        function truncateText(inputText, maxLength) {
            if (inputText.length <= maxLength) {
                return inputText;
            }

            // Attempt to find the last sentence end before maxLength
            const truncated = inputText.slice(0, maxLength);
            const sentenceEndRegex = /[.!?]\s/g;
            let lastSentenceEnd = -1;
            let match;

            while ((match = sentenceEndRegex.exec(truncated)) !== null) {
                lastSentenceEnd = match.index + 1; // Position after the punctuation
            }

            if (lastSentenceEnd !== -1) {
                return truncated.slice(0, lastSentenceEnd).trim();
            }

            // If no sentence end found, find the last space to avoid cutting a word
            const lastSpace = truncated.lastIndexOf(' ');
            if (lastSpace !== -1) {
                return truncated.slice(0, lastSpace).trim();
            }

            // If no space found, hard cut at maxLength
            return truncated;
        }

        // Truncate the input text if necessary
        const processedText = truncateText(text, MAX_CHAR);

        // Optional: Notify if the text was truncated
        if (processedText.length < text.length) {
            console.warn('Input text was truncated to fit the 4096 character limit.');
        }

        try {
            const response = await fetch('https://api.openai.com/v1/audio/speech', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}` // Ensure apiKey is defined and holds your OpenAI API key
                },
                body: JSON.stringify({
                    model: 'tts-1',       // As per OpenAI's latest API documentation
                    input: processedText, // The truncated text to be converted to speech
                    voice: voice          // The voice model to use, defaulting to 'alloy'
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`OpenAI API error: ${response.status} ${response.statusText} - ${JSON.stringify(errorData)}`);
            }

            // Assuming the API returns raw audio data (e.g., MP3)
            const audioBlob = await response.blob();
            const audioUrl = URL.createObjectURL(audioBlob);

            // Manage global audio playback
            if (audio && !audio.paused) {
                audio.pause();
                audio.currentTime = 0;
                console.log('Existing audio playback stopped for new speech.');
            } else if (!audio) {
                // Initialize global audio if not already defined
                audio = new Audio();
            }

            // Set the new audio source
            audio.src = audioUrl;

            // Pause speech recognition when audio starts
            audio.addEventListener('play', () => {
                console.log('Audio playback started.');
            }, { once: true });

            // Resume speech recognition when audio ends
            audio.addEventListener('ended', () => {
                console.log('Audio playback ended.');
                resumeSpeechRecognition();
            }, { once: true });

            // Make sure it doesn't hear itself
            pauseSpeechRecognition();

            // Play the audio
            audio.play();
        } catch (error) {
            console.error('Error synthesizing speech:', error);
            throw error;
        }
    }

    function pauseSpeechRecognition() {
        if (isListening) {
            console.log('Pausing speech recognition.');
            isListening = false;
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }
    }
    
    function resumeSpeechRecognition() {
        if (!isListening && isHandsFree) {
            console.log('Resuming speech recognition.');
            isListening = true;
            listenContinuously();
        }
    }    
</script>

</body>
</html>
