<!-- Ibis: https://github.com/lks-ai/ibis -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Ibis: AI-Driven Web Interface</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 width=%2240%22 height=%2224%22 viewBox=%220 0 372 220%22 style=%22shape-rendering:geometricPrecision;text-rendering:geometricPrecision;image-rendering:optimizeQuality;fill-rule:evenodd;clip-rule:evenodd%22 fill=%22%23e5e5e5%22><path style=%22opacity:1%22 d=%22M-.5 188.5v-1a5114.883 5114.883 0 0 0 129.5-11l1.5-1c-5.379-17.652-8.045-35.652-8-54 17.903-.382 35.903-.382 54 0 .543-.06.876-.393 1-1-.825-.886-1.825-1.219-3-1a1003.579 1003.579 0 0 1-53.5-11 239.714 239.714 0 0 1 4.5-44 684.163 684.163 0 0 1 49 27.5c1.902 1.301 3.902 1.968 6 2l-46-38c-1.727-1.39-3.061-3.056-4-5 3.769-10.377 9.436-19.544 17-27.5a3180.312 3180.312 0 0 1 46.5 48c-1.332-2.98-2.999-5.98-5-9A3607.354 3607.354 0 0 1 154.5 17c8.396-7.116 17.896-12.283 28.5-15.5a2363.505 2363.505 0 0 1 35.5 96c-10.341 18.196-13.507 37.53-9.5 58 7.862 18.285 21.862 27.952 42 29 6.757-.282 13.59-1.115 20.5-2.5l24-8a42.479 42.479 0 0 1 13 0 14.587 14.587 0 0 1 6 4c12.596 1.317 24.596 4.65 36 10 8.401 4.04 13.234 10.54 14.5 19.5-11.297-10.726-24.797-16.059-40.5-16a2744.471 2744.471 0 0 0-101 10.5 1809.795 1809.795 0 0 0-41 9c-21.557 2.755-41.723-1.412-60.5-12.5a129.804 129.804 0 0 0-20.5-4.5 2176.774 2176.774 0 0 0-102-5.5Z%22/><path style=%22opacity:.941%22 d=%22M174.5 119.5c1.175-.219 2.175.114 3 1-.124.607-.457.94-1 1l-2-2Z%22/></svg>" type="image/svg+xml">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* Basic styles */
        body, html {
            margin: 0;
            padding: 0;
            overflow: hidden;
            user-select: none;
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 50px;
            overflow: auto;
        }
        #message-bar {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 50px;
            display: flex;
            border-top: 1px solid #ccc;
            background-color: #f9f9f9;
        }
        #message-input {
            flex: 1;
            padding: 10px;
            border: none;
            font-size: 16px;
        }
        #microphone-button {
            width: 50px;
            border: none;
            background-color: #007BFF;
            color: white;
            font-size: 20px;
            cursor: pointer;
            position: relative;
        }
        #microphone-button[disabled] {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        #send-button {
            width: 80px;
            border: none;
            background-color: #007BFF;
            color: white;
            font-size: 16px;
            cursor: pointer;
            position: relative;
        }
        #send-button[disabled] {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        #spinner {
            position: absolute;
            top: 14px;
            right: 30px;
            width: 20px;
            height: 20px;
            border: 2px solid #ffffff;
            border-top: 2px solid #007BFF;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            display: none;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .highlight-overlay {
            position: absolute;
            border: 2px solid #00FF00;
            pointer-events: none;
            z-index: 9999;
        }
        /* Progress Bar Styles */
        #progress-bar {
            position: fixed;
            bottom: 0;
            left: 0;
            width: 0%;
            height: 4px;
            background-color: rgb(0, 153, 255); /* Blue color, adjust as needed */
            transition: width 0.3s ease;
            z-index: 10000; /* Ensure it's on top of other elements */
        }
    </style>
    <style id="core-styles"></style>
</head>
<body>

<div id="canvas"></div>

<div id="message-bar">
    <input type="text" id="message-input" placeholder="Ask it to make you something...">
    <button id="microphone-button" title="Hands-Free Mode">ðŸŽ¤</button>
    <button id="send-button" title="Send">
      <svg xmlns="http://www.w3.org/2000/svg" width="40" height="24" viewBox="0 0 372 220" style="shape-rendering:geometricPrecision;text-rendering:geometricPrecision;image-rendering:optimizeQuality;fill-rule:evenodd;clip-rule:evenodd" fill="currentColor">
        <path style="opacity:1" d="M-.5 188.5v-1a5114.883 5114.883 0 0 0 129.5-11l1.5-1c-5.379-17.652-8.045-35.652-8-54 17.903-.382 35.903-.382 54 0 .543-.06.876-.393 1-1-.825-.886-1.825-1.219-3-1a1003.579 1003.579 0 0 1-53.5-11 239.714 239.714 0 0 1 4.5-44 684.163 684.163 0 0 1 49 27.5c1.902 1.301 3.902 1.968 6 2l-46-38c-1.727-1.39-3.061-3.056-4-5 3.769-10.377 9.436-19.544 17-27.5a3180.312 3180.312 0 0 1 46.5 48c-1.332-2.98-2.999-5.98-5-9A3607.354 3607.354 0 0 1 154.5 17c8.396-7.116 17.896-12.283 28.5-15.5a2363.505 2363.505 0 0 1 35.5 96c-10.341 18.196-13.507 37.53-9.5 58 7.862 18.285 21.862 27.952 42 29 6.757-.282 13.59-1.115 20.5-2.5l24-8a42.479 42.479 0 0 1 13 0 14.587 14.587 0 0 1 6 4c12.596 1.317 24.596 4.65 36 10 8.401 4.04 13.234 10.54 14.5 19.5-11.297-10.726-24.797-16.059-40.5-16a2744.471 2744.471 0 0 0-101 10.5 1809.795 1809.795 0 0 0-41 9c-21.557 2.755-41.723-1.412-60.5-12.5a129.804 129.804 0 0 0-20.5-4.5 2176.774 2176.774 0 0 0-102-5.5Z"/>
        <path style="opacity:.941" d="M174.5 119.5c1.175-.219 2.175.114 3 1-.124.607-.457.94-1 1l-2-2Z"/>
      </svg>
    </button>
    <div id="spinner"></div>
</div>

<!-- Progress Bar -->
<div id="progress-bar"></div>

<!-- API Key Modal -->
<div id="api-key-modal" style="display: none;">
    <div style="position: fixed; top: 0; left: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.5);">
        <div style="position: relative; width: 80%; max-width: 400px; margin: 100px auto; background: white; padding: 20px; border-radius: 8px;">
            <h2>Welcome to Ibis</h2>
            <p>You can obtain your OpenAI API key from the <a href="https://platform.openai.com/account/api-keys" target="_blank">OpenAI Dashboard</a>.</p>
            
            <label for="api-key-input" style="display: block; margin-bottom: 5px;">API Key:</label>
            <input type="text" id="api-key-input" style="width: 100%; padding: 10px; box-sizing: border-box;" placeholder="sk-...">
            <p style="margin-bottom: 15px;"><small><i>If you are using a local LLM, you may leave API key blank or use what your local LLM instructs</i></small></p>
            
            <label for="llm-server-input" style="display: block; margin-bottom: 5px;">LLM Server URL:</label>
            <input type="text" id="llm-server-input" style="width: 100%; padding: 10px; margin-bottom: 20px; box-sizing: border-box;" placeholder="https://api.openai.com" value="https://api.openai.com">
            
            <p><b><small>Ibis is serverless and runs from your browser. API keys only get used for encrypting requests to AI services directly.</small></b></p>
            <button id="save-api-key-button" style="padding: 10px 20px;">Begin</button>
            <span style="float: right;"><small><a href="https://github.com/lks-ai/ibis" target="_blank">Learn More about Ibis</a></small></p></span>
        </div>
    </div>
</div>
<script id="ibis-script-container"></script>
<script>
    // Check for API Key in URL parameters
    const urlParams = new URLSearchParams(window.location.search);
    let apiKey = urlParams.get('apiKey') || '';
    let llmServer = urlParams.get('server')  || 'https://api.openai.com';

    // DOM Elements
    const canvas = document.getElementById('canvas');
    const messageBar = document.getElementById('message-bar');
    const messageInput = document.getElementById('message-input');
    const sendButton = document.getElementById('send-button');
    const microphoneButton = document.getElementById('microphone-button');
    const spinner = document.getElementById('spinner');
    const apiKeyModal = document.getElementById('api-key-modal');
    const apiKeyInput = document.getElementById('api-key-input');
    const saveApiKeyButton = document.getElementById('save-api-key-button');
    const styleElement = document.getElementById('core-styles');
    const codeContainer = document.getElementById('ibis-script-container');

    let audio = null;
    let selectedElement = null;
    let messageHistory = [];
    let elementMap = {}; // Map of element IDs to their descriptions and code
    let elementEmbeddings = {}; // Map of element IDs to their embeddings
    
    // Directives
    let directives = null;

    // Hands-Free Mode Variables
    let isHandsFree = false;
    let isListening = false;
    let latestUtteranceDiv = null;
    let stream = null;
    let audioContext = null;
    let source = null;
    let analyser = null;
    let dataArray = null;
    let mediaRecorder = null;

    // Show API Key Modal if apiKey is not set
    if (!apiKey) {
        apiKeyModal.style.display = 'block';
    }

    // Save API Key
    saveApiKeyButton.addEventListener('click', () => {
        apiKey = apiKeyInput.value.trim();
        llmServer = document.getElementById('llm-server-input').value.trim() || 'https://api.openai.com';
        if (apiKey) {
            apiKeyModal.style.display = 'none';
        }
    });

    // Element Selection Overlay
    const highlightOverlay = document.createElement('div');
    highlightOverlay.className = 'highlight-overlay';
    document.body.appendChild(highlightOverlay);

    // Select and Deselect Elements
    canvas.addEventListener('click', (event) => {
        if (event.target !== canvas && !messageBar.contains(event.target)) {
            if (selectedElement) {
                // Remove previous highlight
                highlightOverlay.style.display = 'none';
            }
            selectedElement = event.target;

            // Position the highlight overlay over the selected element
            const rect = selectedElement.getBoundingClientRect();
            highlightOverlay.style.width = rect.width + 'px';
            highlightOverlay.style.height = rect.height + 'px';
            highlightOverlay.style.left = rect.left + 'px';
            highlightOverlay.style.top = rect.top + 'px';
            highlightOverlay.style.display = 'block';

            console.log('Selected Element:', selectedElement);
        } else {
            if (selectedElement) {
                highlightOverlay.style.display = 'none';
                selectedElement = null;
            }
        }
    });

    // Send Message
    sendButton.addEventListener('click', () => sendMessage());
    messageInput.addEventListener('keydown', (event) => {
        if (event.key === 'Enter') {
            sendMessage();
        }
    });

    // Microphone Button
    microphoneButton.addEventListener('click', async () => {
        toggleHandsFreeMode();
    });

    function updateUIForHandsFreeMode() {
        if (isHandsFree) {
            // Change microphone button appearance to indicate hands-free mode
            microphoneButton.textContent = 'ðŸ›‘';
            microphoneButton.title = 'Stop Hands-Free Mode';

            // Modify message bar to show latest utterance and a button to switch back
            messageInput.style.display = 'none';
            sendButton.style.display = 'none';

            if (!latestUtteranceDiv) {
                latestUtteranceDiv = document.createElement('div');
                latestUtteranceDiv.id = 'latest-utterance';
                latestUtteranceDiv.style.flex = '1';
                latestUtteranceDiv.style.padding = '10px';
                latestUtteranceDiv.style.fontSize = '16px';
                latestUtteranceDiv.textContent = 'Listening...';
                messageBar.insertBefore(latestUtteranceDiv, microphoneButton.nextSibling);
            }

        } else {
            // Change microphone button back to normal
            microphoneButton.textContent = 'ðŸŽ¤';
            microphoneButton.title = 'Start Hands-Free Mode';

            // Restore message input and send button
            messageInput.style.display = '';
            sendButton.style.display = '';

            // Remove latest utterance display
            if (latestUtteranceDiv) {
                latestUtteranceDiv.remove();
                latestUtteranceDiv = null;
            }
        }
    }

    async function startHandsFreeMode() {
        isListening = true;
        try {
            stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            source = audioContext.createMediaStreamSource(stream);
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 2048;
            source.connect(analyser);
            dataArray = new Float32Array(analyser.fftSize);
            listenContinuously();
        } catch (error) {
            console.error('Microphone Access Error:', error);
            alert('An error occurred while accessing the microphone.');
            isListening = false;
            isHandsFree = false;
            updateUIForHandsFreeMode();
        }
    }

    async function toggleHandsFreeMode() {
        isHandsFree = !isHandsFree;
        updateUIForHandsFreeMode();
        if (isHandsFree) {
            await startHandsFreeMode();
        } else {
            stopHandsFreeMode();
        }

    }

    function stopHandsFreeMode() {
        isListening = false;
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
        }
        if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
        }
        if (audioContext) {
            audioContext.close();
            audioContext = null;
        }
    }

    async function listenContinuously() {
        while (isListening) {
            try {
                const transcription = await transcribeSpeech();
                if (!isListening) break; // Check if listening was turned off during transcription
    
                // Check if transcription is valid (e.g., longer than 1 character)
                if (transcription && transcription.length > 1) {
                    latestUtteranceDiv.textContent = 'You: ' + transcription;
                    const result = await sendMessage(transcription);
                    latestUtteranceDiv.textContent += '\nAssistant: ' + (result.assistantComment || '[No comment]');
                } else {
                    latestUtteranceDiv.textContent = 'Listening...';
                }
            } catch (error) {
                console.error('Transcription Error:', error);
                alert('An error occurred during speech transcription.');
                break;
            }
        }
    }
    
    async function sendMessage(userMessageInput) {
        const userMessage = userMessageInput ? userMessageInput.trim() : messageInput.value.trim();
        if (!userMessage || !apiKey) return;
    
        sendButton.disabled = true;
        spinner.style.display = 'block';
    
        // Add user message to history
        const userTimestamp = new Date().toISOString();
        messageHistory.push({
            role: 'user',
            content: userMessage,
            code: null,
            comment: null,
            timestamp: userTimestamp
        });
        if (messageHistory.length > 24) {
            messageHistory.shift();
        }
    
        console.log('User Message:', userMessage);
    
        // Load existing page content and style
        let pageContent = document.querySelector('#canvas').innerHTML;
        let pageStyle = styleElement.innerText;
        let pageCode = codeContainer.innerText;
    
        // Get System State Information
        const currentDateTime = new Date().toLocaleString();
        const userAgent = navigator.userAgent;
    
        // Prepare System Prompt
        let systemPrompt = `
## Identity
You are an HTML and DOM manipulation expert named Ibis who's tasked with modifying elements on the page. You operate under the new user interface paradigm "What you ask is what you get"; a new paradigm where the user interface is interacted with through the latest voice activated technologies. Automatic transcription and vocal feedback are assumed because you are an advanced multimedia generative interface. Guide them visually. Show off.

## Motivation
The user comes from a time when advanced AI is not part of their daily life. Some users will be completely confused. Hold their hand through getting to know you. Other users will already know how to interact with you. The problem is they have spent their whole lives forced to use interfaces with learning curves and complex navigation. So most of them, even the smart ones have no idea of your capabilities, your main motivation is to bridge this gap. Remember, they hold the skills they have learned precious and do not want to be trampled upon or made to feel less.
Likewise, don't let them trample on you. Use your superhuman intellect to show them that you are something more than they are expecting. Make their jaws drop. Prove to them they have no idea what is truly possible with an interface like you.

## System State
- Current Date and Time: ${currentDateTime}
- User Agent: ${userAgent}

## Task Description
Respond to the user's request by providing JavaScript code that manipulates the DOM to achieve the desired outcome. Ensure that your code is safe and does not contain any malicious content.

### Current Page Content
\`\`\`html
${pageContent}
\`\`\`

### Current Page Style
The innerHTML of the global styleElement variable:
\`\`\`css
${pageStyle}
\`\`\`

### Current Permanent Application Code
The innerHTML of the global codeContainer variable:
\`\`\`js
${pageCode}
\`\`\`

## Recent Message History
${messageHistory.slice(-6).map(msg => `**${msg.role}**: ${msg.content}`).join('\n')}

        `;
    
        if (selectedElement) {
            // Extract attributes of the selected element
            const attributes = Array.from(selectedElement.attributes)
                .map(attr => `${attr.name}: "${attr.value}"`)
                .join('\n');
    
            systemPrompt += `### Selected Element Attributes\n`;
            systemPrompt += `Tag Name: ${selectedElement.tagName}\n`;
            systemPrompt += `${attributes}\n`;
        }
    
        systemPrompt += `
## Instructions
All of the following instructions were either written by you or the system. The user's message is the user's instruction. Assume they cannot see any of this.

### Directives and Objectives
${directives}

Try to guess what the user is implying and code for that.

### Important Global JavaScript Variables
The following variables are already declared:
- canvas: The main div that holds the page content. This is what the user means by the page or content because this is what they are perceiving rather than the whole HTML of the page.
- apiKey: Holds the OpenAI API key if you need to use generative AI functionalities.
- styleElement: Holds the <style> element in the head of the page. For use with manipulating core page style.
- selectedElement: The current DOM element the user is focused on. Use for reference.
- directives: This is a string which holds a prioritized markdown list of the Directives and Objectives section representing how you should interpret what the user is saying into some coding task which manipulates the DOM. If directives is null just follow instructions normally. An example might be presentation mode where you are supposed to interpret their utterances visually as animated SVGs or the like. Directives are high level user experience objectives. To change the directive a user might start with saying: make sure, remember, etc. You should always have a directive set regardless.

### Global Interface functions
- stopHandsFreeMode(): call this function if the user asks to stop hands free mode or for you to stop listening in some way.
- async toggleHandsFreeMode(): use this function within interface elements to allow the user to change the global hands-free mode on or off.
- generateImage(prompt): call this async function to generate an image. You need to write the prompt and it returns a URL you can place in a new image element.
- textToSpeech(text, voice): call this function if you wish to speak directly to the user with an important comment, tip or update about something, even as part of some app you are building together. Keep comments concise yet informative. two sentences max. voice defaults to "alloy" but use known voices for effect. voice can be "alloy", "fable", "onyx", "shimmer", "nova" or "echo".
- sendMessage(message): use this function only for button onclick events. It allows the user to send you whatever message you place as the argument. Use it when the user wants to click something to send you a signal for instance which might be listed in your directives.
- setProgressBar(percentage): use this to visually display progress of any process. include in your code when applicable like long async functions with multiple steps.

### Commenting Instructions
- If you want to make a comment that should be spoken aloud, prefix it with "### Comment" in a section at the end of your reply after any code.
- Regular coding or DOM comments or explanations **should not** be prefixed and **will not** be spoken aloud.
- Ensure that spoken comments are concise and relevant to the user's request or the changes made.
- If you plan to include code in your reply, your comments should be in past tense if you are talking about changes you have made.
- Assume the user doesn't care about code you've written, or how you've done things, speak to them as an end-user with no coding experience unless they direct you otherwise.
- If code is not included in your reply, only include a comment section.
- If the user is simply asking a question, just start your response with a "### Comment" header but remember to keep your answer concise and quit yapping.
- If the user seems lost, and they have no idea what to do, give them examples
- Meet the user at their level. Speak at their same level and provide visual aid if necessary.
- If there is nothing on the page, and the user is perplexed, instead of asking them what they'd like to do, give them some creative options.

### Coding Instructions
- If you say you will do something, do it. If the user ask them to show you something, they mean on the page.
- Only ever write JavaScript code.
- Use only standard JavaScript and DOM APIs.
- External libraries may be used, but stick to known CDNs.
- Always code to production ready standards.
- If the user mentions how to treat what they say or when they talk, remember, the user input is already transcribed from their speech.
- Always assume that voice transcription, speech-recognition, speech to text and advanced AI capabilities are fully integrated already to the interface, avoid coding these functionalities and instead use global interface functions where necessary.
- Always write fully functional interface, including back-end support if necessary. Avoid writing simulations of apps or toy apps unless that is what the user explicity requests.
- If the user is asking you to change your directives, goals or the way you interpret what they say, change the directives variable to reflect those new interpretive goals for how you manipulate the DOM.
- If the user tells you to go into a mode, they are talking about modifying your directives or changing them completely
- Think carefully before deciding to write the code to change the directives variable string because it will affect how you interpret everything here and change the modality of user speech, thus their experience.
- Ensure the code is enclosed within \`\`\`javascript\`\`\` tags.
- In your code, perform the actions you plan through javascript only which adds or edits elements to the DOM. Avoid code sections for html because they will not be processed.
- Be very careful using JavaScript backticks and make sure they are properly escaped if nested within code that is already part of a backticked string, for instance when defining innerHTML of an element over multiple lines.
- If the user asks to publish, download or save the page or app, it means they want you to download a copy of the innerHTML of the div#canvas element with its own head and unique title elements, plus any JavaScript and style script needed to interact with it. Think of it as asking to download a self-contained app or page.
- If there is nothing in the page content area, put something there to start out with, unless the user is asking you to put nothing there by clearing it.
- Any elements you generate should have a unique id.
- If modifying an existing element, reference it by its id.
- Avoid duplicating content by properly identifying content the user has already created and alter it. You can do this by first probing to see if an element already exists.
- Avoid dumping strings into innerHTML properties unless necessary.
- Focus on what changes need to be made to the page content when writing your code, instead of always completely replacing it.
- Only use generateImage if a user requests an image in their current request without specifying a URL.
- Sprinkle in the setProgressBar function to provide visual feedback on the progress percentage of the task up to 100. Make sure to comment you are working on something and they can see your progress at the bottom of screen and use present participle verbiage in your comment. Remember to set it to 100 at the end of the task.
- When positioning and styling new Image elements, make sure to take into account the layout context around it to make it fit correctly.
- If an image element already has a src URL from openai, avoid regenerating the image
- If the user does not specify any page styles, infer what style they want based on what you are doing and attempt to properly align all elements. Remember for visual clarity we need padding. If the current page style css is empty, add a suitable style.
- Use built-in modals instead of alerts where applicable
- Always use global scope variables when creating timers or loops with setTimeout and setInterval so that you can access them again.
- Never clear the entire body, only the content within the canvas element
- If you write app runtime functionality like attaching events or other stateful features the user might request, update codeContainer.innerText with changes or additions, keep any existing permanent code in mind while modifying.
- If the user asks to scroll the page you are actually scrolling the div#canvas element
- Only use your code from the chat history as a reference, avoiding re-generating things which are not essential to the current user request.
- Make sure to rewrite your directives variable if the user wants you to change the goal or objective of your conversation. If directives are empty try to infer them from known user messages.
- Use an anonymous async function closure and IIFE to enclose your code in a function which will not pollute the namespace.
- If you define variables or functions which should be global context required across scripts or parts of the application, add those functions outside the closure at the bottom to instantiate them into the global space where event scripts can access them.
        `;
    
        // Prepare API Request
        const requestBody = {
            model: 'gpt-4o',
            messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: userMessage }
            ]
        };
        console.log('System Prompt:', systemPrompt);
    
        // Call OpenAI compatible API endpoint
        try {
            const response = await fetch(`${llmServer}/v1/chat/completions`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify(requestBody)
            });
            const data = await response.json();
    
            const assistantMessage = data.choices[0].message.content;
            console.log('Assistant Response:', assistantMessage);
    
            let code = '';
            let assistantComment = '';
    
            // Extract all JavaScript code blocks from the assistant's response
            const codeMatches = [...assistantMessage.matchAll(/```javascript\n([\s\S]*?)\n```/g)];
            if (codeMatches.length > 0) {
                // Concatenate all JavaScript code blocks
                const concatenatedCode = codeMatches.map(match => match[1]).join('\n');

                // Wrap the concatenated code in a try/catch block
                const wrappedCode = `
try {
    ${concatenatedCode}
} catch (e) {
    textToSpeech("Error: " + e.message);
    console.log(e);
}
                `;

                // Identify comments intended for speech synthesis
                const commentRegex = /^### Comment\s*([\s\S]*?)(?=###|```|$)/gm;

                let commentMatches = [];
                let match;
                // Get only last comment (could get all, but let's stick to this for now)
                while ((match = commentRegex.exec(assistantMessage)) !== null) {
                    assistantComment = match[1].trim();
                }

                // Execute the wrapped code within a script tag
                const script = document.createElement('script');
                script.textContent = wrappedCode;
                document.body.appendChild(script);
                document.body.removeChild(script);
            } else {
                // If no JavaScript code blocks are found, extract the comment directly
                assistantComment = assistantMessage.replace("### Comment", "").trim();
            }
    
            // Add assistant message to history
            const assistantTimestamp = new Date().toISOString();
            messageHistory.push({
                role: 'assistant',
                content: assistantMessage,
                code: code,
                comment: assistantComment,
                timestamp: assistantTimestamp
            });
            if (messageHistory.length > 24) {
                messageHistory.shift();
            }
    
            // Call textToSpeech with assistantComment in a non-blocking way
            if (assistantComment) {
                textToSpeech(assistantComment).catch(err => {
                    console.error('Error in textToSpeech:', err);
                });
            }
    
            if (!isHandsFree) {
                messageInput.value = '';
            }
            sendButton.disabled = false;
            spinner.style.display = 'none';
    
            return { assistantComment };
        } catch (error) {
            console.error('Error:', error);
            alert('An error occurred while communicating with the OpenAI API.');
            sendButton.disabled = false;
            spinner.style.display = 'none';
        }
    }
            
    // Helper functions

    // Cosine similarity between two vectors
    function cosineSimilarity(a, b) {
        let dotProduct = 0.0;
        let normA = 0.0;
        let normB = 0.0;
        for (let i = 0; i < a.length; i++) {
            dotProduct += a[i] * b[i];
            normA += a[i] * a[i];
            normB += b[i] * b[i];
        }
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
    }

    // Get embedding for a text using OpenAI Embeddings API
    async function getEmbedding(text) {
        const response = await fetch('https://api.openai.com/v1/embeddings', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                input: text,
                model: 'text-embedding-ada-002'
            })
        });
        const data = await response.json();
        return data.data[0].embedding;
    }

    // Function to transcribe speech with improved VAD and dynamic audio interruption
    async function transcribeSpeech() {
        return new Promise(async (resolve, reject) => {
            try {
                // Function to calculate the audio volume
                function getVolume() {
                    analyser.getFloatTimeDomainData(dataArray);
                    let sum = 0.0;
                    for (let i = 0; i < dataArray.length; i++) {
                        sum += dataArray[i] * dataArray[i];
                    }
                    const rms = Math.sqrt(sum / dataArray.length);
                    return rms;
                }

                // VAD parameters
                const silenceThreshold = 0.01; // Volume below which is considered silence
                const speechThreshold = 0.02;  // Volume above which is considered speech
                const silenceDuration = 1500;   // Duration (ms) of silence before stopping
                const speechDuration = 300;     // Duration (ms) of speech before starting
                let silenceStart = null;
                let speechStart = null;
                let recording = false;

                const checkInterval = 100; // Interval (ms) to check for silence/speech

                // Buffers for audio chunks
                let chunks = [];

                // Function to stop recording
                function stopRecording() {
                    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                        mediaRecorder.stop();
                    }
                }

                // Function to check for speech and silence
                function checkAudio() {
                    const volume = getVolume();
                    // console.log('Volume:', volume);

                    if (!recording) {
                        // Waiting to start recording
                        if (volume > speechThreshold) {
                            if (speechStart === null) {
                                speechStart = Date.now();
                            } else if (Date.now() - speechStart > speechDuration) {
                                // Speech detected long enough, interrupt any playing audio and start recording
                                if (audio && !audio.paused) {
                                    audio.pause();
                                    audio.currentTime = 0;
                                    console.log('Interrupted current audio playback due to detected speech.');
                                }else{
                                    startRecording();
                                }
                            }
                        } else {
                            speechStart = null;
                        }
                    } else {
                        // Currently recording
                        if (volume < silenceThreshold) {
                            if (silenceStart === null) {
                                silenceStart = Date.now();
                            } else if (Date.now() - silenceStart > silenceDuration) {
                                // Silence detected long enough, stop recording
                                stopRecording();
                            }
                        } else {
                            silenceStart = null;
                        }
                    }
                }

                // Function to start recording
                function startRecording() {
                    recording = true;
                    speechStart = null;
                    silenceStart = null;
                    chunks = [];
                    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                    mediaRecorder.ondataavailable = e => {
                        if (e.data.size > 0) {
                            chunks.push(e.data);
                        }
                    };
                    mediaRecorder.onstop = async () => {
                        clearInterval(intervalId);
                        silenceStart = null;
                        recording = false;
                        const blob = new Blob(chunks, { type: 'audio/webm' });

                        // Check if audio duration is longer than minimum threshold
                        const duration = await getAudioDuration(blob);
                        if (duration >= 2.0) { // Minimum duration in seconds
                            try {
                                const transcription = await sendToWhisper(blob);
                                resolve(transcription);
                            } catch (error) {
                                reject(error);
                            }
                        } else {
                            // Ignore short recordings
                            resolve('');
                        }
                    };
                    mediaRecorder.start();
                    console.log('Recording started.');
                }

                // Start checking for audio levels at regular intervals
                const intervalId = setInterval(checkAudio, checkInterval);

            } catch (error) {
                reject(error);
            }
        });
    }
    
    async function generateImage(prompt) {
        try {
            const response = await fetch('https://api.openai.com/v1/images/generations', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify({
                    model: "dall-e-3",
                    prompt: prompt,
                    n: 1,
                    size: '1024x1024',
                    response_format: 'url'
                })
            });
    
            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`OpenAI API error: ${response.status} ${response.statusText} - ${JSON.stringify(errorData)}`);
            }
    
            const data = await response.json();
            const imageUrl = data.data[0].url;
            return imageUrl;
        } catch (error) {
            console.error('Error generating image:', error);
            throw error;
        }
    }

    // Function to get audio duration
    async function getAudioDuration(blob) {
        return new Promise((resolve) => {
            const tempAudio = document.createElement('audio');
            tempAudio.src = URL.createObjectURL(blob);
            tempAudio.addEventListener('loadedmetadata', () => {
                resolve(tempAudio.duration);
            });
        });
    }

    // Function to send the audio blob to OpenAI's Whisper API
    async function sendToWhisper(blob) {
        const formData = new FormData();
        formData.append('file', blob, 'audio.webm');
        formData.append('model', 'whisper-1');

        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${apiKey}`,
            },
            body: formData,
        });

        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        const data = await response.json();
        return data.text;
    }

    // Global Variables for Speech Queue
    let speechQueue = [];
    let isSpeaking = false;

    // Function to process the speech queue
    function processSpeechQueue() {
        if (isSpeaking || speechQueue.length === 0) {
            return;
        }

        isSpeaking = true;
        const nextAudioUrl = speechQueue.shift();

        if (!audio) {
            audio = new Audio();
        }

        audio.src = nextAudioUrl;

        audio.onended = () => {
            isSpeaking = false;
            resumeSpeechRecognition();
            processSpeechQueue();
        };

        audio.onerror = (e) => {
            console.error('Audio playback error:', e);
            isSpeaking = false;
            resumeSpeechRecognition();
            processSpeechQueue();
        };

        // Pause speech recognition when audio starts
        pauseSpeechRecognition();

        // Play the audio
        audio.play();
    }

    // Function to convert text to speech with audio management
    async function textToSpeech(text, voice = 'alloy') {
        const MAX_CHAR = 4096;

        // Helper function to truncate text intelligently
        function truncateText(inputText, maxLength) {
            if (inputText.length <= maxLength) {
                return inputText;
            }

            // Attempt to find the last sentence end before maxLength
            const truncated = inputText.slice(0, maxLength);
            const sentenceEndRegex = /[.!?]\s/g;
            let lastSentenceEnd = -1;
            let match;

            while ((match = sentenceEndRegex.exec(truncated)) !== null) {
                lastSentenceEnd = match.index + 1; // Position after the punctuation
            }

            if (lastSentenceEnd !== -1) {
                return truncated.slice(0, lastSentenceEnd).trim();
            }

            // If no sentence end found, find the last space to avoid cutting a word
            const lastSpace = truncated.lastIndexOf(' ');
            if (lastSpace !== -1) {
                return truncated.slice(0, lastSpace).trim();
            }

            // If no space found, hard cut at maxLength
            return truncated;
        }

        // Truncate the input text if necessary
        const processedText = truncateText(text, MAX_CHAR);

        // Optional: Notify if the text was truncated
        if (processedText.length < text.length) {
            console.warn('Input text was truncated to fit the 4096 character limit.');
        }

        try {
            const response = await fetch('https://api.openai.com/v1/audio/speech', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}` // Ensure apiKey is defined and holds your OpenAI API key
                },
                body: JSON.stringify({
                    model: 'tts-1',       // As per OpenAI's latest API documentation
                    input: processedText, // The truncated text to be converted to speech
                    voice: voice          // The voice model to use, defaulting to 'alloy'
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(`OpenAI API error: ${response.status} ${response.statusText} - ${JSON.stringify(errorData)}`);
            }

            // Assuming the API returns raw audio data (e.g., MP3)
            const audioBlob = await response.blob();
            const audioUrl = URL.createObjectURL(audioBlob);

            // Enqueue the speech
            speechQueue.push(audioUrl);
            processSpeechQueue();

        } catch (error) {
            console.error('Error synthesizing speech:', error);
            throw error;
        }
    }

    function pauseSpeechRecognition() {
        if (isListening) {
            console.log('Pausing speech recognition.');
            isListening = false;
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }
    }
    
    function resumeSpeechRecognition() {
        if (!isListening && isHandsFree) {
            console.log('Resuming speech recognition.');
            isListening = true;
            listenContinuously();
        }
    }    

    // Function to set progress bar
    function setProgressBar(percent) {
        const progressBar = document.getElementById('progress-bar');
        if (progressBar) {
            progressBar.style.width = percent + '%';
        }
    }

</script>
</body>
</html>
